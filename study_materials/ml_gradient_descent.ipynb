{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T09:48:49.331694Z",
     "start_time": "2025-03-16T09:48:49.245109Z"
    },
    "id": "dimA924v2QUi"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3076ec-e668-4a04-9315-a937778b6916"
   },
   "source": [
    "## Gradient Descent Types\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Type</th>\n",
    "        <th>Update Frequency</th>\n",
    "        <th>Strengths</th>\n",
    "        <th>Weaknesses</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Batch GD</td>\n",
    "        <td>Entire dataset</td>\n",
    "        <td>Stable convergence</td>\n",
    "        <td>Slow for large data</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>SGD</td>\n",
    "        <td>Single sample</td>\n",
    "        <td>Fast updates</td>\n",
    "        <td>Noisy, less stable</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Mini-Batch GD</td>\n",
    "        <td>Small batch</td>\n",
    "        <td>Balance of BGD & SGD</td>\n",
    "        <td>Needs tuning of batch size</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Momentum GD</td>\n",
    "        <td>Entire dataset</td>\n",
    "        <td>Reduces oscillations</td>\n",
    "        <td>May overshoot minima</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>NAG</td>\n",
    "        <td>Entire dataset</td>\n",
    "        <td>Faster than Momentum</td>\n",
    "        <td>More complex</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Adagrad</td>\n",
    "        <td>Adaptive</td>\n",
    "        <td>No manual tuning</td>\n",
    "        <td>Learning rate decays fast</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>RMSprop</td>\n",
    "        <td>Adaptive</td>\n",
    "        <td>Solves Adagrad decay</td>\n",
    "        <td>Still needs tuning</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Adam</td>\n",
    "        <td>Adaptive + Momentum</td>\n",
    "        <td>Best of both worlds</td>\n",
    "        <td>More computation</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T09:48:49.341860Z",
     "start_time": "2025-03-16T09:48:49.337256Z"
    },
    "id": "8rsFAvsLNAwr"
   },
   "outputs": [],
   "source": [
    "# compute the predicted values using a linear regression model\n",
    "def compute_gradient(x, y, w, b):\n",
    "    m = x.shape[0]\n",
    "    predictions = np.dot(x, w) + b\n",
    "    errors = predictions - y\n",
    "    dj_dw = (1 / m) * np.dot(x.T, errors)\n",
    "    dj_db = (1 / m) * np.sum(errors)\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3ff4ebc-ca0f-4f72-8777-b9bc3b76f55c"
   },
   "source": [
    "### Batch\n",
    "* Computes the gradient using the entire dataset at each iteration:\n",
    "* Equation:\n",
    "$$w = w -  \\alpha \\frac{1}{m}  \\sum\\limits_{i = 0}^{m-1} \\frac{\\partial J(w,b)}{\\partial w}$$\n",
    "\n",
    "* Characteristics:\n",
    "    * More stable convergence\n",
    "    * Computationally expensive for large data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T09:48:49.669763Z",
     "start_time": "2025-03-16T09:48:49.664314Z"
    },
    "id": "a526da4e-4ddc-4a3f-a44e-a3f9b09dd934"
   },
   "outputs": [],
   "source": [
    "def batch_gradient_descent(x, y, w, b, alpha, num_iters):\n",
    "    m = x.shape[0]\n",
    "    for _ in range(num_iters):\n",
    "        dj_dw, dj_db = compute_gradient(x, y, w, b)\n",
    "\n",
    "        w -= alpha * dj_dw\n",
    "        b -= alpha * dj_db\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81b56f77-f1b0-4afd-acab-2ef99d0d385d"
   },
   "source": [
    "### Stochastic (SGD)\n",
    "\n",
    "* Updates the parameters after computing the gradient from a single randomly chosen data point\n",
    "* Equation:\n",
    "$$w = w - \\alpha \\frac{\\partial J(w,b)}{\\partial w}$$\n",
    "\n",
    "* Characteristics:\n",
    "    * Faster updates per iteration\n",
    "    * Noisy updates, which means it's less stable than **Batch GD**\n",
    "    * CAn escape local minima due to randomness\n",
    "    * Suitable for large data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T09:48:49.694312Z",
     "start_time": "2025-03-16T09:48:49.682903Z"
    },
    "id": "d6c61478-9e96-4b26-9c1a-5bf25d1668b7"
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(x, y, w, b, alpha, num_iters):\n",
    "    m = x.shape[0]\n",
    "    for _ in range(num_iters):\n",
    "        i = np.random.randint(m)  # pick a random training example\n",
    "        dj_dw, dj_db = compute_gradient(x[i:i + 1], y[i:i + 1], w, b)  # use only one sample\n",
    "\n",
    "        w -= alpha * dj_dw\n",
    "        b -= alpha * dj_db\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f76650d4-9720-4108-865e-4e6ffff1d70b"
   },
   "source": [
    "### Mini-Batch\n",
    "\n",
    "* Divides the dataset into small batches and computes the gradient using each batch\n",
    "* Equation:\n",
    "$$w = w - \\alpha \\frac{1}{b} \\sum\\limits_{i\\in b} \\frac{\\partial J(w,b)}{\\partial w}$$\n",
    "* where:\n",
    "    * $b$ is the batch size\n",
    "\n",
    "<br>\n",
    "\n",
    "* Characteristics:\n",
    "    * Balances efficiency and stability\n",
    "    * Faster than **Batch GD**\n",
    "    * Less noisy than **SGD**\n",
    "    * Works well in DL apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T09:48:49.708178Z",
     "start_time": "2025-03-16T09:48:49.703302Z"
    },
    "id": "c1210033-4f93-4ded-bffc-48016e03b274"
   },
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(x, y, w, b, alpha, num_iters, batch_size):\n",
    "    m = x.shape[0]\n",
    "    for _ in range(num_iters):\n",
    "        indices = np.random.choice(m, batch_size, replace=False)  # select a batch\n",
    "        x_batch = x[indices]\n",
    "        y_batch = y[indices]\n",
    "        dj_dw, dj_db = compute_gradient(x_batch, y_batch, w, b)\n",
    "\n",
    "        w -= alpha * dj_dw\n",
    "        b -= alpha * dj_db\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d0ab69d-df1e-4c94-9ba8-ffda505861a6"
   },
   "source": [
    "### Momentum-based\n",
    "\n",
    "* Uses past gradients to smooth updates and prevent oscillations\n",
    "* Equation:\n",
    "$$v_w = \\beta v_w + (1- \\beta) \\frac{\\partial J}{\\partial w}$$\n",
    "$$w = w - \\alpha v_w$$\n",
    "\n",
    "* where:\n",
    "    * $v_w$ = momentum of $w$, which is exponentially weighted moving average of past gradients for $w$\n",
    "    * $\\beta$ = momentum coefficient\n",
    "\n",
    "<br>\n",
    "\n",
    "* $\\beta$ poperties:\n",
    "    * Higher `beta` (ex: 0.9 - 0.99):\n",
    "        * More reliance on past gradients (strong momentum)\n",
    "        * Faster convergence but may overshoot the minimum\n",
    "    * Lower `beta` (ex: 0.5 - 0.8)\n",
    "        * Less reliance on past gradients (weak momentum)\n",
    "        * More responsive to sudden gradient changes\n",
    "        * Might be better for noisy or non-stationary problems\n",
    "\n",
    "<br>\n",
    "\n",
    "* Characteristics:\n",
    "    * Helps in faster convergence\n",
    "    * Reduces oscillations in high-dimensional spaces\n",
    "    * Commonly used in DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T09:48:49.726050Z",
     "start_time": "2025-03-16T09:48:49.718993Z"
    },
    "id": "c24a0d99-d9c4-4519-bf1a-386ba4eeda0e"
   },
   "outputs": [],
   "source": [
    "def momentum_gradient_descent(x, y, w, b, alpha, num_iters, beta=0.9):\n",
    "    v_w, v_b = 0, 0  # initialize momentum variables\n",
    "    for _ in range(num_iters):\n",
    "        dj_dw, dj_db = compute_gradient(x, y, w, b)\n",
    "        v_w = beta * v_w + (1 - beta) * dj_dw\n",
    "        v_b = beta * v_b + (1 - beta) * dj_db\n",
    "        w -= alpha * v_w\n",
    "        b -= alpha * v_b\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6ee742d-4201-4e81-8a08-496192759972"
   },
   "source": [
    "### Nesterov Accelerated Gradient (NAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "193c5553-3fc9-40f6-b2fb-350981bd15c5"
   },
   "source": [
    "### Adagrad (Adaptive Gradient Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66ef1577-1bc3-454a-9343-019d8ac1fec0"
   },
   "source": [
    "### RMSprop (Root Mean Square Propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c99cba88-82a1-4dbc-b3d8-20f4a43487a3"
   },
   "source": [
    "### Adam (Adaptive Moment Estimation)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMrnpTwatTvD7DQo0+hgH84",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
